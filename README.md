# Light Open R1

*è¿™æ˜¯ä¸€ä¸ªè½»é‡çº§çš„åŸºäºOpen R1å¤ç°DeepSeek-R1çš„é¡¹ç›®ã€‚ä¸»è¦ç”¨äºç³»ç»Ÿçš„ç†è§£å’Œå­¦ä¹ DeepSeek-R1æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹*

## ç¡¬ä»¶é…ç½®
æˆ‘çš„ç¡¬ä»¶é…ç½®ä¸ºå•å¡RTX 3060 12Gï¼Œä»¥ä¸‹å‘½ä»¤éƒ½èƒ½æˆåŠŸè¿è¡Œï¼Œæ‰§è¡Œæ—¶é—´åŸºäºæˆ‘çš„3060æ˜¾å¡ï¼Œä»…ä¾›å‚è€ƒ

æ®è¯´ä½¿ç”¨unslothå¯ä»¥è¿è¡Œæ›´å¤§å‚æ•°çš„æ¨¡å‹ï¼Œå¦‚æœå¤§å®¶å–œæ¬¢çš„åŒ–è®°å¾—ä¸€é”®ä¸‰è¿ï¼Œæˆ‘åç»­åˆ†äº«å®è·µç»“æœ

## ç¯å¢ƒé…ç½®
```shell
conda create -n light-open-r1 python=3.11 -y
conda activate light-open-r1
pip install vllm==0.7.1
pip install wandb
pip install flash-attn --no-build-isolation
pip install -e .
apt-get install git-lfs
#æŒ‰éœ€ä¿®æ”¹ä¸ºä½ çš„proxyåœ°å€ï¼Œå¦åˆ™æ— æ³•æ‹‰å–å’Œæ¨é€æ¨¡å‹
export HTTP_PROXY="http://192.168.4.86:4780"
export HTTPS_PROXY="http://192.168.4.86:4780"
huggingface-cli login
wandb login
```

## æ‰§è¡Œè®­ç»ƒ
```shell
# æ‰§è¡ŒSFTè®­ç»ƒï¼Œæ‰§è¡Œæ—¶é—´å¤§çº¦445åˆ†é’Ÿï¼Œæ³¨æ„è§‚å¯Ÿæ˜¾å¡æ¸©åº¦
python src/open_r1/sft.py --config recipes/Qwen2.5-0.5B-Instruct-light/grpo/config_demo.yaml
```


<img src="assets/image-1.png" width="500">
<img src="assets/image.png" width="500">




```shell
# æ‰§è¡ŒGRPOè®­ç»ƒï¼Œæ—¶é—´è¾ƒé•¿ï¼Œæ‰§è¡Œä¸­æˆªå›¾å¦‚ä¸‹
python src/open_r1/grpo.py --config recipes/Qwen2.5-0.5B-Instruct-light/grpo/config_demo.yaml
```
<img src="assets/image-2.png" width="1000">


## åç»­
å¹³å¸¸å·¥ä½œè¾ƒå¿™ï¼ŒæŠ½ç©ºæ›´æ–°åç»­éƒ¨åˆ†


ä»¥ä¸‹ä¸ºåŸOpen R1é¡¹ç›®å†…å®¹ç¿»è¯‘
---

**ç›®å½•**  
1. [æ¦‚è¿°](#overview)  
2. [æ”»å‡»è®¡åˆ’](#plan-of-attack)  
3. [å®‰è£…](#installation)  
4. [è®­ç»ƒæ¨¡å‹](#training-models)  
   - [SFT](#sft)  
   - [GRPO](#grpo)  
5. [è¯„ä¼°æ¨¡å‹](#evaluating-models)  
6. [å¤ç°Deepseekåœ¨MATH-500ä¸Šçš„è¯„ä¼°ç»“æœ](#reproducing-deepseeks-evaluation-results-on-math-500)  
7. [æ•°æ®ç”Ÿæˆ](#data-generation)  
   - [ä»å°å‹è’¸é¦R1æ¨¡å‹ç”Ÿæˆæ•°æ®](#generate-data-from-a-smol-distilled-r1-model)  
   - [ä»DeepSeek-R1ç”Ÿæˆæ•°æ®](#generate-data-from-deepseek-r1)  
8. [è´¡çŒ®](#contributing)

## æ¦‚è¿°

è¿™ä¸ªä»“åº“çš„ç›®æ ‡æ˜¯æ„å»ºR1æµç¨‹ä¸­ç¼ºå¤±çš„éƒ¨åˆ†,ä½¿å¾—æ¯ä¸ªäººéƒ½èƒ½å¤ç°å¹¶åœ¨å…¶åŸºç¡€ä¸Šè¿›è¡Œå¼€å‘ã€‚è¯¥é¡¹ç›®è®¾è®¡ç®€å•,ä¸»è¦åŒ…å«:

- `src/open_r1`: åŒ…å«ç”¨äºè®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹ä»¥åŠç”Ÿæˆåˆæˆæ•°æ®çš„è„šæœ¬:
    - `grpo.py`: åœ¨ç»™å®šæ•°æ®é›†ä¸Šä½¿ç”¨GRPOè®­ç»ƒæ¨¡å‹ã€‚
    - `sft.py`: åœ¨æ•°æ®é›†ä¸Šå¯¹æ¨¡å‹è¿›è¡Œç®€å•çš„SFTã€‚
    - `evaluate.py`: åœ¨R1åŸºå‡†æµ‹è¯•ä¸Šè¯„ä¼°æ¨¡å‹ã€‚
    - `generate.py`: ä½¿ç”¨[Distilabel](https://github.com/argilla-io/distilabel)ä»æ¨¡å‹ç”Ÿæˆåˆæˆæ•°æ®ã€‚
- `Makefile`: åŒ…å«åˆ©ç”¨ä¸Šè¿°è„šæœ¬æ‰§è¡ŒR1æµç¨‹ä¸­æ¯ä¸ªæ­¥éª¤çš„æ˜“ç”¨å‘½ä»¤ã€‚

### æ”»å‡»è®¡åˆ’

æˆ‘ä»¬å°†ä½¿ç”¨DeepSeek-R1çš„[æŠ€æœ¯æŠ¥å‘Š](https://github.com/deepseek-ai/DeepSeek-R1)ä½œä¸ºæŒ‡å—,å¤§è‡´å¯ä»¥åˆ†ä¸ºä¸‰ä¸ªä¸»è¦æ­¥éª¤:

* æ­¥éª¤1:é€šè¿‡ä»DeepSeek-R1è’¸é¦é«˜è´¨é‡è¯­æ–™åº“æ¥å¤ç°R1-Distillæ¨¡å‹ã€‚
* æ­¥éª¤2:å¤ç°DeepSeekç”¨äºåˆ›å»ºR1-Zeroçš„çº¯RLæµç¨‹ã€‚è¿™å¯èƒ½éœ€è¦ä¸ºæ•°å­¦ã€æ¨ç†å’Œä»£ç é¢†åŸŸç­–åˆ’æ–°çš„å¤§è§„æ¨¡æ•°æ®é›†ã€‚
* æ­¥éª¤3:å±•ç¤ºæˆ‘ä»¬å¯ä»¥é€šè¿‡å¤šé˜¶æ®µè®­ç»ƒä»åŸºç¡€æ¨¡å‹åˆ°RLè°ƒä¼˜ã€‚

<center>
    <img src="assets/plan-of-attack.png" width="500">
</center>


## å®‰è£…

**æ³¨æ„:åº“ä¾èµ–äºCUDA 12.4ã€‚å¦‚æœé‡åˆ°æ®µé”™è¯¯,è¯·æ£€æŸ¥æ‚¨çš„ç³»ç»Ÿã€‚**

è¦è¿è¡Œæ­¤é¡¹ç›®ä¸­çš„ä»£ç ,é¦–å…ˆä½¿ç”¨ä¾‹å¦‚`uv`åˆ›å»ºä¸€ä¸ªPythonè™šæ‹Ÿç¯å¢ƒã€‚
è¦å®‰è£…`uv`,è¯·å‚ç…§[UVå®‰è£…æŒ‡å—](https://docs.astral.sh/uv/getting-started/installation/)ã€‚

```shell
uv venv openr1 --python 3.11 && source openr1/bin/activate && uv pip install --upgrade pip --link-mode=copy
```

æ¥ä¸‹æ¥,å®‰è£…vLLM:

```shell
uv pip install vllm==0.7.1 --link-mode=copy
```

è¿™ä¹Ÿä¼šå®‰è£…PyTorch `v2.5.1`,ä½¿ç”¨è¿™ä¸ªç‰ˆæœ¬**éå¸¸é‡è¦**,å› ä¸ºvLLMçš„äºŒè¿›åˆ¶æ–‡ä»¶æ˜¯ä¸ºå…¶ç¼–è¯‘çš„ã€‚ç„¶å,æ‚¨å¯ä»¥é€šè¿‡`pip install -e .[LIST OF MODES]`ä¸ºæ‚¨çš„ç‰¹å®šç”¨ä¾‹å®‰è£…å…¶ä½™ä¾èµ–é¡¹ã€‚å¯¹äºå¤§å¤šæ•°è´¡çŒ®è€…,æˆ‘ä»¬å»ºè®®:

```shell
GIT_LFS_SKIP_SMUDGE=1 uv pip install -e ".[dev]" --link-mode=copy
```

æ¥ä¸‹æ¥,æŒ‰å¦‚ä¸‹æ–¹å¼ç™»å½•æ‚¨çš„Hugging Faceå’ŒWeights and Biasesè´¦æˆ·:

```shell
huggingface-cli login
wandb login
```

æœ€å,æ£€æŸ¥æ‚¨çš„ç³»ç»Ÿæ˜¯å¦å®‰è£…äº†Git LFS,ä»¥ä¾¿æ‚¨å¯ä»¥åŠ è½½å’Œæ¨é€æ¨¡å‹/æ•°æ®é›†åˆ°Hugging Face Hub:

```shell
git-lfs --version
```

å¦‚æœæœªå®‰è£…,è¿è¡Œ:

```shell
sudo apt-get install git-lfs
```

## è®­ç»ƒæ¨¡å‹

æˆ‘ä»¬æ”¯æŒä½¿ç”¨DDPæˆ–DeepSpeed(ZeRO-2å’ŒZeRO-3)è®­ç»ƒæ¨¡å‹ã€‚ä¾‹å¦‚,è¦åœ¨ä»DeepSeek-R1è’¸é¦çš„å¸¦æœ‰æ¨ç†ç—•è¿¹çš„æ•°æ®é›†(å¦‚[Bespoke-Stratos-17k](https://huggingface.co/datasets/bespokelabs/Bespoke-Stratos-17k))ä¸Šè¿è¡ŒSFT,æ‰§è¡Œ:

```shell
# é€šè¿‡å‘½ä»¤è¡Œè®­ç»ƒ
accelerate launch --config_file=recipes/accelerate_configs/zero3.yaml src/open_r1/sft.py \
    --model_name_or_path Qwen/Qwen2.5-1.5B-Instruct \
    --dataset_name HuggingFaceH4/Bespoke-Stratos-17k \
    --learning_rate 2.0e-5 \
    --num_train_epochs 1 \
    --packing \
    --max_seq_length 4096 \
    --per_device_train_batch_size 2 \
    --gradient_accumulation_steps 8 \
    --gradient_checkpointing \
    --bf16 \
    --output_dir data/Qwen2.5-1.5B-Open-R1-Distill

# é€šè¿‡YAMLé…ç½®è®­ç»ƒ
accelerate launch --config_file recipes/accelerate_configs/zero3.yaml src/open_r1/sft.py \
    recipes/Qwen/Qwen2.5-1.5B-Instruct/sft/config_demo.yaml
```

ç›®å‰æ”¯æŒä»¥ä¸‹ä»»åŠ¡:

* ç›‘ç£å¾®è°ƒ `sft`
* ç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ– `grpo`

> [!TIP]
> å¦‚æœæ‚¨å¢åŠ /å‡å°‘GPUæ•°é‡,æˆ‘ä»¬å»ºè®®åŒæ—¶è°ƒæ•´æ¯è®¾å¤‡æ‰¹é‡å¤§å°æˆ–æ¢¯åº¦ç´¯ç§¯æ­¥æ•°,ä»¥ä¿æŒå…¨å±€æ‰¹é‡å¤§å°ä¸å˜ã€‚

é»˜è®¤æƒ…å†µä¸‹,è¿™äº›è„šæœ¬ä¼šå°†æ¯ä¸ªæ¨¡å‹æ¨é€åˆ°æ‚¨çš„Hugging Face Hubç”¨æˆ·åä¸‹,å³`{username}/{model_name}-{task}`ã€‚æ‚¨å¯ä»¥é€šè¿‡åœ¨å‘½ä»¤åé™„åŠ å‚æ•°æ¥è¦†ç›–æ¯ä¸ªYAMLé…ç½®ä¸­çš„å‚æ•°:

```shell
# æ›´æ”¹æ‰¹é‡å¤§å°ã€è®­ç»ƒè½®æ•°ç­‰
accelerate launch --config_file recipes/accelerate_configs/zero3.yaml src/open_r1/sft.py \
    recipes/Qwen/Qwen2.5-1.5B-Instruct/sft/config_demo.yaml
    --per_device_train_batch_size=1 --num_train_epochs=5
```

> [!NOTE]
> ä¸‹é¢çš„è®­ç»ƒå‘½ä»¤æ˜¯ä¸º8 x H100s (80GB)çš„èŠ‚ç‚¹é…ç½®çš„ã€‚å¯¹äºä¸åŒçš„ç¡¬ä»¶å’Œæ‹“æ‰‘ç»“æ„,æ‚¨å¯èƒ½éœ€è¦è°ƒæ•´æ‰¹é‡å¤§å°å’Œæ¢¯åº¦ç´¯ç§¯æ­¥æ•°ã€‚

### SFT

è¦åœ¨ä»DeepSeek-R1è’¸é¦çš„å¸¦æœ‰æ¨ç†ç—•è¿¹çš„æ•°æ®é›†(å¦‚[Bespoke-Stratos-17k](https://huggingface.co/datasets/bespokelabs/Bespoke-Stratos-17k))ä¸Šè¿è¡ŒSFT,æ‰§è¡Œ:

```shell
ACCELERATE_LOG_LEVEL=info accelerate launch --config_file recipes/accelerate_configs/zero3.yaml \
    src/open_r1/sft.py \
    --config recipes/Qwen2.5-1.5B-Instruct/sft/config_demo.yaml
```

### GRPO

è¦é€šè¿‡GRPOè®­ç»ƒå™¨è¿›è¡Œè®­ç»ƒ,æˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªGPUè¿è¡ŒvLLMä»¥åŠ å¿«ç”Ÿæˆé€Ÿåº¦,å…¶ä½™GPUç”¨äºè®­ç»ƒã€‚ä¾‹å¦‚,åœ¨8ä¸ªGPUçš„èŠ‚ç‚¹ä¸Š,ä½¿ç”¨`recipes/accelerate_configs/zero3.yaml`é…ç½®,ç„¶åè¦†ç›–`num_processes`ä»¥åœ¨7ä¸ªè®¾å¤‡ä¸Šè¿è¡Œ:

```shell
ACCELERATE_LOG_LEVEL=info accelerate launch --config_file recipes/accelerate_configs/zero3.yaml \
    --num_processes=7 src/open_r1/grpo.py \
    --config recipes/Qwen2.5-1.5B-Instruct/grpo/config_demo.yaml
```

æˆ‘ä»¬æä¾›äº†ä¸€ä¸ªä½¿ç”¨GRPOè¿›è¡Œæ•°å­¦æ¨ç†çš„æœ€å°å¯å¤ç°å®éªŒ,å‚è€ƒäº†[SimpleRL-Reason](https://hkust-nlp.notion.site/simplerl-reason)çš„æ–¹æ³•,è¯¥æ–¹æ³•ä½¿ç”¨åœ¨8Kä¸ªç¤ºä¾‹ä¸Šè®­ç»ƒçš„7Bæ¨¡å‹ã€‚åœ¨8ä¸ªH100 80G GPUä¸Šè¿è¡Œå¤§çº¦éœ€è¦3å°æ—¶:

```shell
ACCELERATE_LOG_LEVEL=info accelerate launch --config_file recipes/accelerate_configs/zero2.yaml \
    --num_processes=7 src/open_r1/grpo.py \
    --config recipes/Qwen2.5-Math-7B/grpo/config_simple_rl.yaml
```

æˆ‘ä»¬çš„æœ€ç»ˆ[æ¨¡å‹](https://huggingface.co/Dongwei/Qwen-2.5-7B_Base_Math_smalllr),è™½ç„¶ä½¿ç”¨äº†ä¸åŒçš„å­¦ä¹ ç‡ã€æŸå¤±å‡½æ•°å’Œå¥–åŠ±ç»“æ„,åœ¨MATH-500ä¸Šè¾¾åˆ°äº†69.4%çš„å‡†ç¡®ç‡,ç›¸æ¯”åŸºç¡€æ¨¡å‹æé«˜äº†17%ä»¥ä¸Šã€‚

### åœ¨Slurmé›†ç¾¤ä¸Šå¯åŠ¨ä½œä¸š

å¦‚æœæ‚¨å¯ä»¥è®¿é—®Slurmé›†ç¾¤,æˆ‘ä»¬æä¾›äº†ä¸€ä¸ª`slurm/train.slurm`è„šæœ¬,å¯ä»¥è‡ªåŠ¨ä¸ºæ‚¨æ’é˜Ÿè®­ç»ƒä½œä¸šã€‚ä»¥ä¸‹æ˜¯ä½¿ç”¨æ–¹æ³•:

```shell
sbatch --job-name=open_r1 --nodes=1 slurm/train.slurm {model_name} {task} {config_suffix} {accelerator}
```

è¿™é‡Œ`{model_name}`å’Œ`{task}`å¦‚ä¸Šå®šä¹‰,è€Œ`{config_suffix}`æŒ‡ç‰¹å®šé…ç½®,`{accelerator}`æŒ‡`recipes/accelerate_configs`ä¸­çš„ğŸ¤— Accelerateé…ç½®é€‰æ‹©ã€‚å¦‚æœæ‚¨æƒ³è¦†ç›–é»˜è®¤é…ç½®å‚æ•°,å¯ä»¥é€šè¿‡é™„åŠ ä¸€ä¸ªç©ºæ ¼åˆ†éš”çš„å­—ç¬¦ä¸²,å¦‚`'--arg1=value1 --arg2=value2'`ã€‚ä»¥ä¸‹æ˜¯åœ¨1ä¸ªèŠ‚ç‚¹8ä¸ªGPUä¸Šè¿è¡ŒSFTçš„å…·ä½“ç¤ºä¾‹:

```shell
# åœ¨Slurmä¸Šå¯åŠ¨å¹¶è¦†ç›–é»˜è®¤è¶…å‚æ•°
sbatch --job-name=open_r1 --nodes=1 slurm/train.slurm Qwen2.5-1.5B-Instruct sft demo zero3 '--per_device_train_batch_size=1 --num_train_epochs=5'
```

æ‚¨å¯ä»¥é€šè¿‡å¢åŠ `--nodes`æ ‡å¿—æ¥æ‰©å±•èŠ‚ç‚¹æ•°é‡ã€‚

> [!NOTE]
> `slurm/train.slurm`ä¸­çš„é…ç½®æ˜¯ä¸ºHugging Faceè®¡ç®—é›†ç¾¤ä¼˜åŒ–çš„,å¯èƒ½éœ€è¦è°ƒæ•´ä»¥é€‚åº”æ‚¨è‡ªå·±çš„è®¡ç®—èŠ‚ç‚¹ã€‚

## è¯„ä¼°æ¨¡å‹

æˆ‘ä»¬ä½¿ç”¨`lighteval`æ¥è¯„ä¼°æ¨¡å‹,åœ¨`src/open_r1/evaluate.py`ä¸­å®šä¹‰äº†è‡ªå®šä¹‰ä»»åŠ¡ã€‚å¯¹äºé€‚åˆå•ä¸ªGPUçš„æ¨¡å‹,è¿è¡Œ:

```shell
MODEL=deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B
MODEL_ARGS="pretrained=$MODEL,dtype=bfloat16,max_model_length=32768,gpu_memory_utilisation=0.8"
OUTPUT_DIR=data/evals/$MODEL

# AIME 2024
TASK=aime24
lighteval vllm $MODEL_ARGS "custom|$TASK|0|0" \
    --custom-tasks src/open_r1/evaluate.py \
    --use-chat-template \
    --output-dir $OUTPUT_DIR

# MATH-500
TASK=math_500
lighteval vllm $MODEL_ARGS "custom|$TASK|0|0" \
    --custom-tasks src/open_r1/evaluate.py \
    --use-chat-template \
    --output-dir $OUTPUT_DIR

# GPQA Diamond
TASK=gpqa:diamond
lighteval vllm $MODEL_ARGS "custom|$TASK|0|0" \
    --custom-tasks src/open_r1/evaluate.py \
    --use-chat-template \
    --output-dir $OUTPUT_DIR 
```

> [!IMPORTANT]
> æ‚¨å¿…é¡»åœ¨`vllm`å‘½ä»¤ä¸­è®¾ç½®`max_model_length=32768`ä»¥ä¸æˆ‘ä»¬ä¸ºæ¯ä¸ªè¯„ä¼°å®šä¹‰çš„`generation_size`å¯¹é½ã€‚å¦åˆ™,`lighteval`å°†æŠ›å‡ºé”™è¯¯ã€‚

è¦åœ¨å¤šä¸ªGPUä¸Šæé«˜ååé‡,ä½¿ç”¨_æ•°æ®å¹¶è¡Œ_å¦‚ä¸‹:

```shell
NUM_GPUS=8
MODEL=deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B
MODEL_ARGS="pretrained=$MODEL,dtype=bfloat16,data_parallel_size=$NUM_GPUS,max_model_length=32768,gpu_memory_utilisation=0.8"
TASK=aime24
OUTPUT_DIR=data/evals/$MODEL

lighteval vllm $MODEL_ARGS "custom|$TASK|0|0" \
    --custom-tasks src/open_r1/evaluate.py \
    --use-chat-template \
    --output-dir $OUTPUT_DIR 
```

å¯¹äºéœ€è¦åœ¨GPUä¹‹é—´åˆ†ç‰‡çš„å¤§å‹æ¨¡å‹,ä½¿ç”¨_å¼ é‡å¹¶è¡Œ_å¹¶è¿è¡Œ:

```shell
NUM_GPUS=8
MODEL=deepseek-ai/DeepSeek-R1-Distill-Qwen-32B
MODEL_ARGS="pretrained=$MODEL,dtype=bfloat16,tensor_parallel_size=$NUM_GPUS,max_model_length=32768,gpu_memory_utilisation=0.8"
TASK=aime24
OUTPUT_DIR=data/evals/$MODEL

export VLLM_WORKER_MULTIPROC_METHOD=spawn
lighteval vllm $MODEL_ARGS "custom|$TASK|0|0" \
    --custom-tasks src/open_r1/evaluate.py \
    --use-chat-template \
    --output-dir $OUTPUT_DIR 
```

æ‚¨ä¹Ÿå¯ä»¥ä½¿ç”¨`make evaluate`å¯åŠ¨è¯„ä¼°,æŒ‡å®šæ¨¡å‹ã€ä»»åŠ¡,ä»¥åŠå¯é€‰çš„å¹¶è¡ŒæŠ€æœ¯å’ŒGPUæ•°é‡ã€‚

åœ¨å•ä¸ªGPUä¸Šè¯„ä¼°:

```shell
make evaluate MODEL=deepseek-ai/DeepSeek-R1-Distill-Qwen-32B TASK=aime24
```

ä½¿ç”¨æ•°æ®å¹¶è¡Œ:

```shell
make evaluate MODEL=deepseek-ai/DeepSeek-R1-Distill-Qwen-32B TASK=aime24 PARALLEL=data NUM_GPUS=8
```

ä½¿ç”¨å¼ é‡å¹¶è¡Œ:

```shell
make evaluate MODEL=deepseek-ai/DeepSeek-R1-Distill-Qwen-32B TASK=aime24 PARALLEL=tensor NUM_GPUS=8
```

## å¤ç°Deepseekçš„è¯„ä¼°ç»“æœ

> [!NOTE]
> DeepSeek-R1è®ºæ–‡ä½¿ç”¨æ¸©åº¦ä¸º0.6ã€top-på€¼ä¸º0.95çš„é‡‡æ ·,æ¯ä¸ªæŸ¥è¯¢64ä¸ªå“åº”æ¥ä¼°è®¡`pass@1`ã€‚ä»¥ä¸‹æˆ‘ä»¬æŠ¥å‘Šçš„æ˜¯è´ªå©ªè§£ç çš„ç»“æœ,è¿™å¯èƒ½è§£é‡Šäº†æˆ‘ä»¬çš„ç»“æœä¸ä»–ä»¬çš„ç»“æœä¹‹é—´1-3Ïƒçš„å°å·®å¼‚ã€‚

### MATH-500

æˆ‘ä»¬èƒ½å¤Ÿåœ¨1-3ä¸ªæ ‡å‡†å·®å†…å¤ç°Deepseekåœ¨MATH-500åŸºå‡†æµ‹è¯•ä¸ŠæŠ¥å‘Šçš„ç»“æœ:

| æ¨¡å‹                         | MATH-500 (ğŸ¤— LightEval) | MATH-500 (DeepSeekæŠ¥å‘Š) |
|:------------------------------|:-----------------------:|:----------------------------:|
| DeepSeek-R1-Distill-Qwen-1.5B |          81.2           |             83.9             |
| DeepSeek-R1-Distill-Qwen-7B   |          91.8           |             92.8             |
| DeepSeek-R1-Distill-Qwen-14B  |          94.2           |             93.9             |
| DeepSeek-R1-Distill-Qwen-32B  |          95.0           |             94.3             |
| DeepSeek-R1-Distill-Llama-8B  |          85.4           |             89.1             |
| DeepSeek-R1-Distill-Llama-70B |          93.4           |             94.5             |

è¦å¤ç°è¿™äº›ç»“æœ,ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤:

```shell
NUM_GPUS=1 # å¯¹äº32Bå’Œ70Bæ¨¡å‹è®¾ç½®ä¸º8
MODEL=deepseek-ai/{model_name}
MODEL_ARGS="pretrained=$MODEL,dtype=bfloat16,max_model_length=32768,gpu_memory_utilisation=0.8,tensor_parallel_size=$NUM_GPUS"
OUTPUT_DIR=data/evals/$MODEL

lighteval vllm $MODEL_ARGS "custom|math_500|0|0" \
    --custom-tasks src/open_r1/evaluate.py \
    --use-chat-template \
    --output-dir $OUTPUT_DIR
```

æˆ–è€…,æ‚¨å¯ä»¥æŒ‰å¦‚ä¸‹æ–¹å¼å¯åŠ¨Slurmä½œä¸š:

```shell
python scripts/run_benchmarks.py --model-id={model_id}  --benchmarks math_500
```

### GPQA Diamond

æˆ‘ä»¬èƒ½å¤Ÿåœ¨1-3ä¸ªæ ‡å‡†å·®å†…å¤ç°Deepseekåœ¨GPQA DiamondåŸºå‡†æµ‹è¯•ä¸ŠæŠ¥å‘Šçš„ç»“æœ:

| æ¨¡å‹                         | GPQA Diamond (ğŸ¤— LightEval) | GPQA Diamond (DeepSeekæŠ¥å‘Š) |
|:------------------------------|:---------------------------:|:--------------------------------:|
| DeepSeek-R1-Distill-Qwen-1.5B |            33.3             |               33.8               |
| DeepSeek-R1-Distill-Qwen-7B   |            48.4             |               49.1               |
| DeepSeek-R1-Distill-Qwen-14B  |            55.6             |               59.1               |
| DeepSeek-R1-Distill-Qwen-32B  |            58.6             |               62.1               |
| DeepSeek-R1-Distill-Llama-8B  |            51.0             |               49.0               |
| DeepSeek-R1-Distill-Llama-70B |            65.2             |               65.2               |

è¦å¤ç°è¿™äº›ç»“æœ,ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤:

```shell
NUM_GPUS=1 # å¯¹äº32Bå’Œ70Bæ¨¡å‹è®¾ç½®ä¸º8
MODEL=deepseek-ai/{model_name}
MODEL_ARGS="pretrained=$MODEL,dtype=bfloat16,max_model_length=32768,gpu_memory_utilisation=0.8,tensor_parallel_size=$NUM_GPUS"
OUTPUT_DIR=data/evals/$MODEL

lighteval vllm $MODEL_ARGS "custom|gpqa:diamond|0|0" \
    --custom-tasks src/open_r1/evaluate.py \
    --use-chat-template \
    --output-dir $OUTPUT_DIR
```

```shell
python scripts/run_benchmarks.py --model-id={model_id}  --benchmarks gpqa
```

## æ•°æ®ç”Ÿæˆ

### ä»å°å‹è’¸é¦R1æ¨¡å‹ç”Ÿæˆæ•°æ®

ä»¥ä¸‹ç¤ºä¾‹å¯ä»¥åœ¨1xH100ä¸Šè¿è¡Œã€‚
é¦–å…ˆå®‰è£…ä»¥ä¸‹ä¾èµ–:

```shell
uv pip install "distilabel[vllm]>=1.5.2"
```

ç°åœ¨å°†ä»¥ä¸‹ä»£ç ç‰‡æ®µä¿å­˜åˆ°åä¸º`pipeline.py`çš„æ–‡ä»¶ä¸­,å¹¶ç”¨`python pipeline.py`è¿è¡Œã€‚å®ƒå°†ä¸º10ä¸ªç¤ºä¾‹ä¸­çš„æ¯ä¸€ä¸ªç”Ÿæˆ4ä¸ªè¾“å‡º(å°†ä»“åº“çš„ç”¨æˆ·åæ›´æ”¹ä¸ºæ‚¨çš„ç»„ç»‡/ç”¨æˆ·å):

```python
from datasets import load_dataset
from distilabel.models import vLLM
from distilabel.pipeline import Pipeline
from distilabel.steps.tasks import TextGeneration


prompt_template = """\
You will be given a problem. Please reason step by step, and put your final answer within \boxed{}:
{{ instruction }}"""

dataset = load_dataset("AI-MO/NuminaMath-TIR", split="train").select(range(10))

model_id = "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B"  # Exchange with another smol distilled r1

with Pipeline(
    name="distill-qwen-7b-r1",
    description="A pipeline to generate data from a distilled r1 model",
) as pipeline:

    llm = vLLM(
        model=model_id,
        tokenizer=model_id,
        extra_kwargs={
            "tensor_parallel_size": 1,
            "max_model_len": 8192,
        },
        generation_kwargs={
            "temperature": 0.6,
            "max_new_tokens": 8192,
        },
    )
    prompt_column = "problem"
    text_generation = TextGeneration(
        llm=llm, 
        template=prompt_template,
        num_generations=4,
        input_mappings={"instruction": prompt_column} if prompt_column is not None else {}
    )


if __name__ == "__main__":
    distiset = pipeline.run(dataset=dataset)
    distiset.push_to_hub(repo_id="username/numina-deepseek-r1-qwen-7b")
```

æŸ¥çœ‹[HuggingFaceH4/numina-deepseek-r1-qwen-7b](https://huggingface.co/datasets/HuggingFaceH4/numina-deepseek-r1-qwen-7b)çš„ç¤ºä¾‹æ•°æ®é›†ã€‚

### ä»DeepSeek-R1ç”Ÿæˆæ•°æ®

è¦è¿è¡Œæ›´å¤§çš„DeepSeek-R1,æˆ‘ä»¬ä½¿ç”¨äº†2ä¸ªèŠ‚ç‚¹,æ¯ä¸ªèŠ‚ç‚¹æœ‰8Ã—H100 GPU,ä½¿ç”¨æœ¬ä»“åº“ä¸­`slurm/generate.slurm`çš„slurmæ–‡ä»¶ã€‚é¦–å…ˆ,å®‰è£…ä¾èµ–é¡¹:

(ç›®å‰æˆ‘ä»¬éœ€è¦å®‰è£…[ä¿®å¤R1 cudaå›¾æ•è·](https://github.com/vllm-project/vllm/commits/221d388cc5a836fa189305785ed7e887cea8b510/csrc/moe/moe_align_sum_kernels.cu)çš„vllmå¼€å‘ç‰ˆè½®å­)
```shell
pip install https://wheels.vllm.ai/221d388cc5a836fa189305785ed7e887cea8b510/vllm-1.0.0.dev-cp38-abi3-manylinux1_x86_64.whl --extra-index-url https://download.pytorch.org/whl/cu121

uv pip install "distilabel[vllm,ray,openai]>=1.5.2"
```

ç„¶åè¿è¡Œä»¥ä¸‹å‘½ä»¤:

```shell
sbatch slurm/generate.slurm \
    --hf-dataset AI-MO/NuminaMath-TIR \
    --temperature 0.6 \
    --prompt-column problem \
    --model deepseek-ai/DeepSeek-R1 \
    --hf-output-dataset username/r1-dataset
```

> [!NOTE]  
> åœ¨ä½œä¸šè¿è¡Œæ—¶,æ‚¨å¯ä»¥é€šè¿‡é›†ç¾¤ç™»å½•èŠ‚ç‚¹è®¾ç½®SSHéš§é“æ¥ä»æ‚¨çš„è®¡ç®—æœºè®¿é—®Rayä»ªè¡¨æ¿,è¿è¡Œ`ssh -L 8265:ray_ip_head_node:8265 <login_node>`,ç„¶åæµè§ˆ`http://localhost:8265`

## è´¡çŒ®

æ¬¢è¿è´¡çŒ®ã€‚è¯·å‚è€ƒ https://github.com/huggingface/open-r1/issues/23ã€‚
